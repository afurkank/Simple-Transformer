{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWSq1ZN0MjIf",
        "outputId": "dd2dde65-67c9-4527-e476-68154b1064a5"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers\n",
        "!pip install torchdata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from typing import Tuple\n",
        "from torch import Tensor\n",
        "from torchtext.datasets import Multi30k\n",
        "from torch.utils.data import DataLoader\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1dD0NMEMjIh",
        "outputId": "e8bbac94-b93b-42b1-bf60-bb011631acc8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l7QQ1zDWMjIj"
      },
      "outputs": [],
      "source": [
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "f = open(\"parallelcorpus.txt\", \"a\")\n",
        "\n",
        "for i in train_iter:\n",
        "  for x in [x.rstrip(\"\\n\") for x in i]:\n",
        "    f.write(x)\n",
        "    f.write(' ')\n",
        "\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AO-nW7opMjIk"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 64\n",
        "VOCAB_SIZE = 32768\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[PAD]\", \"[BOS]\", \"[EOS]\"])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.train(['parallelcorpus.txt'], trainer)\n",
        "\n",
        "tokenizer.enable_padding(pad_id=1, length=MAX_LEN)\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[BOS] $A [EOS]\",\n",
        "    special_tokens=[\n",
        "        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
        "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
        "    ],\n",
        ")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_enc = tokenizer.encode(src_sample.rstrip(\"\\n\"))\n",
        "        src_batch.append(torch.tensor(src_enc.ids))\n",
        "\n",
        "        tgt_enc = tokenizer.encode(tgt_sample.rstrip(\"\\n\"))\n",
        "        tgt_batch.append(torch.tensor(tgt_enc.ids))\n",
        "    return torch.stack(src_batch), torch.stack(tgt_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JUJ3J_o1MjIl"
      },
      "outputs": [],
      "source": [
        "test_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, drop_last=True)\n",
        "test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W3rOmg4YMjIl"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size=32678, d_model=512, pad_mask=1):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model, padding_idx=pad_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        return self.emb(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XuvYlXDuMjIm"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_h\n",
        "        self.d_ff = d_ff\n",
        "        self.d_k = self.d_v = int(d_model / d_h)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.Linear = nn.Linear(d_model, d_model)\n",
        "        self.normalize1 = nn.LayerNorm(d_model)\n",
        "        self.normalize2 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.linears = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(d_h*3)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        multi_head = []\n",
        "        for i in range(self.d_h):\n",
        "            query = self.linears[3*i](x)\n",
        "            key = self.linears[3*i + 1](x)\n",
        "            value = self.linears[3*i + 2](x)\n",
        "            scaledDotProd = (query @ key.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
        "            soft = F.softmax(scaledDotProd, dim=-1)\n",
        "            soft = self.dropout(soft)\n",
        "            attn =  soft @ value\n",
        "            multi_head.append(attn)\n",
        "        selfAttn = self.Linear(torch.cat((multi_head), -1))\n",
        "        addNorm = self.normalize1(x + selfAttn)\n",
        "        encoderOutput = self.normalize2(x + self.feed_forward(addNorm))\n",
        "        return encoderOutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cfHw7_VuMjIn"
      },
      "outputs": [],
      "source": [
        "class EncoderStack(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8, dropout=0.1, N=6):\n",
        "        super(EncoderStack, self).__init__()\n",
        "        self.encoders = nn.ModuleList([EncoderLayer(d_model, d_ff, d_h, dropout) for _ in range(N)]) # Stacking Encoder Layer N Times\n",
        "\n",
        "    def forward(self, x):\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gaZLO3lsMjIn"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_h\n",
        "        self.d_k = self.d_v = int(self.d_model / self.d_h)\n",
        "        self.linears = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(d_h*3)])\n",
        "        self.firstLinear = nn.Linear(d_h * self.d_v, d_model)\n",
        "        self.secondLinear = nn.Linear(d_h * self.d_model, d_model)\n",
        "        self.normalize1 = nn.LayerNorm(d_model)\n",
        "        self.normalize2 = nn.LayerNorm(d_model)\n",
        "        self.normalize3 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, y, y_mask=None):\n",
        "        multi_head1 = []\n",
        "        multi_head2 = []\n",
        "\n",
        "        # FIRST ATTENTION LAYER\n",
        "        ''' Same as encoder, but here we have tgt(target) as the decoder's input '''\n",
        "        for i in range(self.d_h):\n",
        "            query = self.linears[3*i](y)\n",
        "            key = self.linears[3*i+1](y)\n",
        "            value = self.linears[3*i+2](y)\n",
        "            scaledDotProd = (query @ key.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
        "            if y_mask is not None:\n",
        "                scaledDotProd = scaledDotProd.masked_fill(y_mask==0, float('-inf'))\n",
        "            soft = F.softmax(scaledDotProd, dim=-1)\n",
        "            soft = self.dropout1(soft)\n",
        "            attn =  soft @ value\n",
        "            multi_head1.append(attn)\n",
        "        selfAttn = self.firstLinear(torch.cat((multi_head1), dim=-1))\n",
        "        addNorm1 = self.normalize1(y + selfAttn)\n",
        "\n",
        "        # SECOND ATTENTION LAYER\n",
        "        ''' Attention layer, instead of V and K matrices, we use the output of the encoder '''\n",
        "        for i in range(self.d_h):\n",
        "            scaledDotProd = addNorm1 @ x.transpose(-1, -2) / math.sqrt(self.d_k)\n",
        "            soft = F.softmax(scaledDotProd, dim=-1)\n",
        "            soft = self.dropout2(soft)\n",
        "            attn = soft @ x\n",
        "            multi_head2.append(attn)\n",
        "        crossAttn = self.secondLinear(torch.cat((multi_head2), dim=-1))\n",
        "        addNorm2 = self.normalize2(y + crossAttn)\n",
        "        decoderOutput = self.normalize3(y + self.feed_forward(addNorm2))\n",
        "        return decoderOutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NFSeeoKTMjIo"
      },
      "outputs": [],
      "source": [
        "class DecoderStack(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8, dropout=0.1, N=6):\n",
        "        super(DecoderStack, self).__init__()\n",
        "        self.decoders = nn.ModuleList([DecoderLayer(d_model, d_ff, d_h, dropout) for _ in range(N)]) # Stacking Decoder Layer N Times\n",
        "\n",
        "    def forward(self, x, y, y_mask=None):\n",
        "        for decoder in self.decoders:\n",
        "            y = decoder(x, y, y_mask)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Tx4vpT4725w3"
      },
      "outputs": [],
      "source": [
        "class Mask(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mask, self).__init__()\n",
        "    \n",
        "    def forward(self, batch_size, seq_len1, seq_len2):\n",
        "        mask = torch.tril(torch.ones(batch_size, seq_len1, seq_len2)).to(device)\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "P4V-2pak25w4"
      },
      "outputs": [],
      "source": [
        "class Positional_Encoding(nn.Module):\n",
        "    def __init__(self, d_model=512, n=10000):\n",
        "        super(Positional_Encoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n = n\n",
        "    def forward(self, seq_len):\n",
        "        P = torch.zeros(seq_len, self.d_model)\n",
        "        for k in range(seq_len):\n",
        "            for i in range(self.d_model // 2):\n",
        "                denominator = math.pow(self.n, 2*i/self.d_model)\n",
        "                P[k, 2*i] = math.sin(k/denominator)\n",
        "                P[k, 2*i+1] = math.cos(k/denominator)\n",
        "        P = P.to(device)\n",
        "        return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Eejkpbku25wz"
      },
      "outputs": [],
      "source": [
        "class ModelOutput(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super(ModelOutput, self).__init__()\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lsNHTQTPMjIo"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model=512, d_h = 8, d_ff=2048, vocab_size=32768, dropout=0.1, num_coder_layers=6):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_h\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "        self.generate = ModelOutput(d_model, vocab_size)\n",
        "        self.dropoutEnc = nn.Dropout(dropout)\n",
        "        self.dropoutDec = nn.Dropout(dropout)\n",
        "        self.embed = Embedding(vocab_size, d_model, pad_mask=1)\n",
        "        self.positional = Positional_Encoding(self.d_model, 10000)\n",
        "        self.masking = Mask()\n",
        "        self.encoderStack = EncoderStack(d_model, d_ff, d_h, dropout, num_coder_layers)\n",
        "        self.decoderStack = DecoderStack(d_model, d_ff, d_h, dropout, num_coder_layers)\n",
        "\n",
        "    def forward(self, x: Tensor, y: Tensor):\n",
        "        assert x.shape[0] == y.shape[0]\n",
        "        batch_dim = x.shape[0]\n",
        "        assert x.shape[1] == y.shape[1]\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        y_mask = self.masking(batch_dim, seq_len, seq_len)\n",
        "\n",
        "        x, y = self.embed(x), self.embed(y)\n",
        "\n",
        "        pos_encoding = self.positional(seq_len)\n",
        "\n",
        "        x = pos_encoding + x\n",
        "        y = pos_encoding + y\n",
        "\n",
        "        x, y = self.dropoutEnc(x), self.dropoutDec(y)\n",
        "\n",
        "        encoderOutput = self.encoderStack(x)\n",
        "\n",
        "        decoderOutput = self.decoderStack(encoderOutput, y, y_mask)\n",
        "\n",
        "        logits = self.generate(decoderOutput)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "n5c2hP21MjIp"
      },
      "outputs": [],
      "source": [
        "model = Transformer(d_model=512, d_h=8, d_ff=2048, vocab_size=VOCAB_SIZE, dropout=0.3, num_coder_layers=3).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3OfIu8IFjmbt"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, train_data: DataLoader, test_data: DataLoader, learning_rate: int, padding_idx: int, epoch_num: int):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
        "\n",
        "    train_loss = []\n",
        "\n",
        "    for epoch in range(0, epoch_num):\n",
        "\n",
        "        print(\"#\"*67)\n",
        "        print(f'{\"#\"*20}Training begins for epoc {epoch:>2}{\"#\"*20}')\n",
        "        print(\"#\"*67)\n",
        "        loss_arr=[]\n",
        "        total_loss = 0\n",
        "        for i, (src, tgt) in enumerate(train_data):\n",
        "\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            probs = model(src, tgt).permute(0, 2, 1)\n",
        "            loss = criterion(probs, tgt)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i%100 == 0 and i != 0:\n",
        "                loss_per_batch = total_loss / 100\n",
        "                print(f'Loss per batch is: {loss_per_batch}')\n",
        "                loss_arr.append(loss_per_batch)\n",
        "                train_loss.append(loss_per_batch)\n",
        "                total_loss = 0\n",
        "        \n",
        "        print(f'Training is complete for epoch {epoch:>2}, average training loss for epoch {epoch:>2}: {np.average(loss_arr):>5.3f}')\n",
        "        print(\"-\"*67)\n",
        "    print(\"Loss graph: \")\n",
        "    plt.plot(train_loss, color='blue')\n",
        "    plt.title(\"Loss Graph\")\n",
        "    plt.xlabel(\"Batch\")\n",
        "    plt.ylabel(\"Loss per Batch\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JEYHRiFeZVwT",
        "outputId": "ddc3887b-6544-490d-ea66-035f80476d50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###################################################################\n",
            "####################Training begins for epoc  0####################\n",
            "###################################################################\n",
            "Loss per batch is: 2.3146367114782334\n",
            "Loss per batch is: 0.7558378687500954\n",
            "Loss per batch is: 0.5250715824961663\n",
            "Loss per batch is: 0.39204117462038995\n",
            "Loss per batch is: 0.31017629101872446\n",
            "Loss per batch is: 0.27458895564079283\n",
            "Loss per batch is: 0.22657060079276561\n",
            "Loss per batch is: 0.20447475016117095\n",
            "Loss per batch is: 0.20920431099832057\n",
            "Training is complete for epoch  0, average training loss for epoch  0: 0.579\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc  1####################\n",
            "###################################################################\n",
            "Loss per batch is: 0.08189887491753325\n",
            "Loss per batch is: 0.05732614059234038\n",
            "Loss per batch is: 0.04712578350910917\n",
            "Loss per batch is: 0.039721727567957714\n",
            "Loss per batch is: 0.02974977665580809\n",
            "Loss per batch is: 0.026457319655455648\n",
            "Loss per batch is: 0.020715178612153978\n",
            "Loss per batch is: 0.018421439603553153\n",
            "Loss per batch is: 0.016181102179689332\n",
            "Training is complete for epoch  1, average training loss for epoch  1: 0.038\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc  2####################\n",
            "###################################################################\n",
            "Loss per batch is: 0.003612570184559445\n",
            "Loss per batch is: 0.0009418702620314435\n",
            "Loss per batch is: 0.0004310740825167159\n",
            "Loss per batch is: 0.00022737025923561305\n",
            "Loss per batch is: 0.00016568984206969618\n",
            "Loss per batch is: 0.00015870785937295296\n",
            "Loss per batch is: 0.00014972524331824388\n",
            "Loss per batch is: 0.00012824410852772418\n",
            "Loss per batch is: 0.00013796841420116835\n",
            "Training is complete for epoch  2, average training loss for epoch  2: 0.001\n",
            "-------------------------------------------------------------------\n",
            "Loss graph: \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "train(model, train_dataloader, test_dataloader, learning_rate=0.001, padding_idx=PAD_IDX, epoch_num=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL1Ka42zXlfq",
        "outputId": "15aadfbb-257d-477c-aa2f-e2f97beeae39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 64])\n",
            "torch.Size([32, 64])\n"
          ]
        }
      ],
      "source": [
        "src, tgt = next(iter(test_dataloader))\n",
        "print(src.shape)\n",
        "print(tgt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WI6E17_CXlfr"
      },
      "outputs": [],
      "source": [
        "logits = model(src, tgt)\n",
        "pred = torch.argmax(logits, dim=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtqy9lxxXlfr",
        "outputId": "1f57b65d-a49f-4064-ce80-28f4e9e33625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example no 0: \n",
            "################################################################################\n",
            "#German text:  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
            "################################################################################\n",
            "#English translation:  A group of men are loading cotton onto a truck\n",
            "################################################################################\n",
            "#Model translation:  A group of men are loading cotton onto a truck\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 1: \n",
            "################################################################################\n",
            "#German text:  Ein Mann schläft in einem grünen Raum auf einem Sofa .\n",
            "################################################################################\n",
            "#English translation:  A man sleeping in a green room on a couch .\n",
            "################################################################################\n",
            "#Model translation:  A man sleeping in a green room on a couch .\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 2: \n",
            "################################################################################\n",
            "#German text:  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau .\n",
            "################################################################################\n",
            "#English translation:  A boy wearing headphones sits on a woman ' s shoulders .\n",
            "################################################################################\n",
            "#Model translation:  A boy wearing headphones sits on a woman ' s shoulders .\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 3: \n",
            "################################################################################\n",
            "#German text:  Zwei Männer bauen eine blaue Eis f ischer hütte auf einem zuge froren en See auf\n",
            "################################################################################\n",
            "#English translation:  Two men setting up a blue ice fishing hut on an iced over lake\n",
            "################################################################################\n",
            "#Model translation:  Two men setting up a blue ice fishing hut on an iced over lake\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 4: \n",
            "################################################################################\n",
            "#German text:  Ein Mann mit beginnender Glatze , der eine rote Rettungsweste trägt , sitzt in einem kleinen Boot .\n",
            "################################################################################\n",
            "#English translation:  A balding man wearing a red life jacket is sitting in a small boat .\n",
            "################################################################################\n",
            "#Model translation:  A balding man wearing a red life jacket is sitting in a small boat .\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 5: \n",
            "################################################################################\n",
            "#German text:  Eine Frau in einem rotem Mantel , die eine vermutlich aus Asien st ammen de Handtasche in einem blauen Farbton hält , springt für einen Schnappschuss in die Luft .\n",
            "################################################################################\n",
            "#English translation:  A lady in a red coat , holding a blu ish hand bag likely of asian descent , jumping off the ground for a snap shot .\n",
            "################################################################################\n",
            "#Model translation:  A lady in a red coat , holding a blu ish hand bag likely of asian descent , jumping off the ground for a snap shot .\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 6: \n",
            "################################################################################\n",
            "#German text:  Ein brauner Hund rennt dem schwarzen Hund hinterher .\n",
            "################################################################################\n",
            "#English translation:  A brown dog is running after the black dog .\n",
            "################################################################################\n",
            "#Model translation:  A brown dog is running after the black dog .\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 7: \n",
            "################################################################################\n",
            "#German text:  Ein kleiner Junge mit einem Giants - Trikot schwingt einen Baseballschläger in Richtung eines ankommenden Balls .\n",
            "################################################################################\n",
            "#English translation:  A young boy wearing a Giants jersey swings a baseball bat at an incoming pitch .\n",
            "################################################################################\n",
            "#Model translation:  A young boy wearing a Giants jersey swings a baseball bat at an incoming pitch .\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 8: \n",
            "################################################################################\n",
            "#German text:  Ein Mann telefoniert in einem unaufgeräumten Büro\n",
            "################################################################################\n",
            "#English translation:  A man in a cluttered office is using the telephone\n",
            "################################################################################\n",
            "#Model translation:  A man in a cluttered office is using the telephone\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 9: \n",
            "################################################################################\n",
            "#German text:  Eine lächelnde Frau mit einem pfirsichfarbenen Trägershirt hält ein Mountainbike\n",
            "################################################################################\n",
            "#English translation:  A smiling woman in a peach tank top stands holding a mountain bike\n",
            "################################################################################\n",
            "#Model translation:  A smiling woman in a peach tank top stands holding a mountain bike\n",
            "################################################################################\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for example in range(10):\n",
        "    print(f\"Example no {example}: \")\n",
        "    pad_count = (tgt[example] == PAD_IDX).data.sum().item()\n",
        "    sentence_len = tgt.shape[1] - pad_count\n",
        "    print(\"#\"*80)\n",
        "    print(\"#German text: \", tokenizer.decode(src[example].cpu().numpy().squeeze()))\n",
        "    print(\"#\"*80)\n",
        "    print(\"#English translation: \", tokenizer.decode(tgt[example].cpu().numpy().squeeze()))\n",
        "    print(\"#\"*80)\n",
        "    print(\"#Model translation: \", tokenizer.decode(pred[example][:sentence_len].cpu().numpy().squeeze()))\n",
        "    print(\"#\"*80)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPgMJ93d25w7"
      },
      "outputs": [],
      "source": [
        "def translate(model: nn.Module, source: Tensor, start_token: int, stop_token: int, seq_len: int):\n",
        "    src_pos_enc = model.positional(source.shape[1]).unsqueeze(0)\n",
        "\n",
        "    src_embedding = model.embed(source)\n",
        "\n",
        "    encoderInput = src_embedding + src_pos_enc\n",
        "\n",
        "    encoderOutput = model.encoderStack(encoderInput)\n",
        "\n",
        "    sequence = torch.tensor([[start_token]], dtype=torch.int32)\n",
        "\n",
        "    for _ in range(30):\n",
        "        embedded_sequence = model.embed(sequence)\n",
        "\n",
        "        seq_pos_enc = model.positional(sequence.shape[1]).unsqueeze(0)\n",
        "\n",
        "        decoderInput = embedded_sequence + seq_pos_enc\n",
        "\n",
        "        decoderInputMask = model.masking(sequence.shape[0], sequence.shape[1], sequence.shape[1])\n",
        "\n",
        "        decoderOutput = model.decoderStack(encoderOutput, decoderInput)\n",
        "\n",
        "        logits = decoderOutput[:, -1, :]\n",
        "\n",
        "        logits = model.generate(logits)\n",
        "\n",
        "        generated_word_id = torch.argmax(logits, dim=-1)\n",
        "        generated_word_tensor = torch.tensor([[generated_word_id.item()]], dtype=torch.int32)\n",
        "\n",
        "        sequence = torch.cat((sequence, generated_word_tensor), dim=1)\n",
        "\n",
        "        if generated_word_id == stop_token:\n",
        "            break\n",
        "        \n",
        "    print(sequence)\n",
        "\n",
        "    translation_ids = sequence.cpu().numpy()\n",
        "\n",
        "    translation = tokenizer.decode(translation_ids.squeeze())\n",
        "    return translation\n",
        "\n",
        "translation = translate(model=model, source=src, start_token=2, stop_token=EOS_IDX, seq_len=MAX_LEN)\n",
        "print(translation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6fb3815109fae185a2989dc67116c03680cdd5befd2823d06e43fcf705655cc7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
