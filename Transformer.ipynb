{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWSq1ZN0MjIf",
        "outputId": "832516d2-47d9-43ff-fc39-a0a057ef6f26"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers\n",
        "!pip install torchdata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from typing import Tuple\n",
        "from torch import Tensor\n",
        "from torchtext.datasets import Multi30k\n",
        "from torch.utils.data import DataLoader\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M1dD0NMEMjIh",
        "outputId": "65f81ce3-b9c2-4669-88a8-5c5a8d147ab3"
      },
      "outputs": [],
      "source": [
        "'''Hyperparameters'''\n",
        "MAX_LEN = 64\n",
        "VOCAB_SIZE = 32768\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "BATCH_SIZE = 64\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "D_MODEL = 512\n",
        "D_H = 8\n",
        "D_FF = 2048\n",
        "EMBEDDING_SIZE = 512\n",
        "N = 3\n",
        "DROPOUT = 0.1\n",
        "LR = 0.0001\n",
        "EPOCHS = 20\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l7QQ1zDWMjIj"
      },
      "outputs": [],
      "source": [
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "f = open(\"parallelcorpus.txt\", \"a\")\n",
        "\n",
        "for i in train_iter:\n",
        "  for x in [x.rstrip(\"\\n\") for x in i]:\n",
        "    f.write(x)\n",
        "    f.write(' ')\n",
        "\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AO-nW7opMjIk"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[PAD]\", \"[BOS]\", \"[EOS]\"])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.train(['parallelcorpus.txt'], trainer)\n",
        "\n",
        "tokenizer.enable_padding(pad_id=1, length=MAX_LEN)\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[BOS] $A [EOS]\",\n",
        "    special_tokens=[\n",
        "        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
        "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
        "    ],\n",
        ")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_enc = tokenizer.encode(src_sample.rstrip(\"\\n\"))\n",
        "        src_batch.append(torch.tensor(src_enc.ids))\n",
        "\n",
        "        tgt_enc = tokenizer.encode(tgt_sample.rstrip(\"\\n\"))\n",
        "        tgt_batch.append(torch.tensor(tgt_enc.ids))\n",
        "    return torch.stack(src_batch), torch.stack(tgt_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JUJ3J_o1MjIl"
      },
      "outputs": [],
      "source": [
        "test_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, drop_last=True)\n",
        "test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W3rOmg4YMjIl"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size=32678, embedding_size=512, pad_mask=1):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, padding_idx=pad_mask, device=DEVICE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x) * math.sqrt(self.embedding_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XuvYlXDuMjIm"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.d_h = d_h\n",
        "        self.d_k = d_model // d_h\n",
        "        self.linears = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(d_h*3)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.Linear = nn.Linear(d_model, d_model)\n",
        "        self.normalize1 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.normalize2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, x_mask=None):\n",
        "        multi_head = []\n",
        "        for i in range(self.d_h):\n",
        "            query = self.linears[3*i](x)\n",
        "            key = self.linears[3*i + 1](x)\n",
        "            value = self.linears[3*i + 2](x)\n",
        "            scaledDotProd = (query @ key.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
        "            if x_mask is not None:\n",
        "                scaledDotProd = scaledDotProd.masked_fill(x_mask==0, float('-inf'))\n",
        "            soft = F.softmax(scaledDotProd, dim=-1)\n",
        "            soft = self.dropout(soft)\n",
        "            attn =  soft @ value\n",
        "            multi_head.append(attn)\n",
        "        selfAttn = self.Linear(torch.cat((multi_head), -1))\n",
        "        addNorm = self.normalize1(x + selfAttn)\n",
        "        encoderOutput = self.normalize2(x + self.feed_forward(addNorm))\n",
        "        return encoderOutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cfHw7_VuMjIn"
      },
      "outputs": [],
      "source": [
        "class EncoderStack(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8, dropout=0.1, N=6):\n",
        "        super(EncoderStack, self).__init__()\n",
        "        self.encoders = nn.ModuleList([EncoderLayer(d_model, d_ff, d_h, dropout) for _ in range(N)]) # Stacking Encoder Layer N Times\n",
        "\n",
        "    def forward(self, x, x_mask=None):\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x, x_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gaZLO3lsMjIn"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.d_h = d_h\n",
        "        self.d_k = d_model // d_h\n",
        "        self.linears = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(d_h*3)])\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.firstLinear = nn.Linear(d_h * self.d_k, d_model)\n",
        "        self.normalize1 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.secondLinear = nn.Linear(d_h * d_model, d_model)\n",
        "        self.normalize2 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.normalize3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, y, y_mask=None):\n",
        "        multi_head1 = []\n",
        "        multi_head2 = []\n",
        "\n",
        "        # FIRST ATTENTION LAYER\n",
        "        ''' Same as encoder, but here we have tgt(target) as the decoder's input '''\n",
        "        for i in range(self.d_h):\n",
        "            query = self.linears[3*i](y)\n",
        "            key = self.linears[3*i+1](y)\n",
        "            value = self.linears[3*i+2](y)\n",
        "            scaledDotProd = (query @ key.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
        "            if y_mask is not None:\n",
        "                scaledDotProd = scaledDotProd.masked_fill(y_mask==0, float('-inf'))\n",
        "            soft = F.softmax(scaledDotProd, dim=-1)\n",
        "            soft = self.dropout1(soft)\n",
        "            attn =  soft @ value\n",
        "            multi_head1.append(attn)\n",
        "        selfAttn = self.firstLinear(torch.cat((multi_head1), dim=-1))\n",
        "        addNorm1 = self.normalize1(y + selfAttn)\n",
        "\n",
        "        # SECOND ATTENTION LAYER\n",
        "        ''' Attention layer, instead of V and K matrices, we use the output of the encoder '''\n",
        "        for i in range(self.d_h):\n",
        "            scaledDotProd = (addNorm1 @ x.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
        "            soft = F.softmax(scaledDotProd, dim=-1)\n",
        "            soft = self.dropout2(soft)\n",
        "            attn = soft @ x\n",
        "            multi_head2.append(attn)\n",
        "        crossAttn = self.secondLinear(torch.cat((multi_head2), dim=-1))\n",
        "        addNorm2 = self.normalize2(y + crossAttn)\n",
        "        decoderOutput = self.normalize3(y + self.feed_forward(addNorm2))\n",
        "        return decoderOutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NFSeeoKTMjIo"
      },
      "outputs": [],
      "source": [
        "class DecoderStack(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, d_h=8, dropout=0.1, N=6):\n",
        "        super(DecoderStack, self).__init__()\n",
        "        self.decoders = nn.ModuleList([DecoderLayer(d_model, d_ff, d_h, dropout) for _ in range(N)]) # Stacking Decoder Layer N Times\n",
        "\n",
        "    def forward(self, x, y, y_mask=None):\n",
        "        for decoder in self.decoders:\n",
        "            y = decoder(x, y, y_mask)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Tx4vpT4725w3"
      },
      "outputs": [],
      "source": [
        "class Mask(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mask, self).__init__()\n",
        "    \n",
        "    def forward(self, batch_dim, seq_len):\n",
        "        mask = torch.tril(torch.ones((batch_dim, seq_len, seq_len), device=DEVICE))\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P4V-2pak25w4"
      },
      "outputs": [],
      "source": [
        "class Positional_Encoding(nn.Module):\n",
        "    def __init__(self, embedding_size=512, n=10000):\n",
        "        super(Positional_Encoding, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n = n\n",
        "    def forward(self, seq_len):\n",
        "        P = torch.zeros(seq_len, self.embedding_size, device=DEVICE)\n",
        "        for k in range(seq_len):\n",
        "            for i in range(self.embedding_size // 2):\n",
        "                denominator = math.pow(self.n, 2*i/self.embedding_size)\n",
        "                P[k, 2*i] = math.sin(k/denominator)\n",
        "                P[k, 2*i+1] = math.cos(k/denominator)\n",
        "        return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Eejkpbku25wz"
      },
      "outputs": [],
      "source": [
        "class GenerateLogits(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super(GenerateLogits, self).__init__()\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lsNHTQTPMjIo"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model=512, d_h = 8, d_ff=2048, embedding_size=512, vocab_size=32768, dropout=0.1, num_coder_layers=6):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed = Embedding(vocab_size, embedding_size, pad_mask=1)\n",
        "        self.positional = Positional_Encoding(embedding_size, 10000)\n",
        "        self.masking = Mask()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.encoderStack = EncoderStack(d_model, d_ff, d_h, dropout, num_coder_layers)\n",
        "        self.decoderStack = DecoderStack(d_model, d_ff, d_h, dropout, num_coder_layers)\n",
        "        self.generate = GenerateLogits(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x: Tensor, y: Tensor):\n",
        "        assert x.shape[0] == y.shape[0]\n",
        "        batch_dim = x.shape[0]\n",
        "        src_seq_len, tgt_seq_len = x.shape[1], y.shape[1]\n",
        "        x_pos_encoding = self.positional(src_seq_len)\n",
        "        y_pos_encoding = self.positional(tgt_seq_len)\n",
        "        x_mask = self.masking(batch_dim, src_seq_len)\n",
        "        y_mask = self.masking(batch_dim, tgt_seq_len)\n",
        "        x, y = self.embed(x), self.embed(y)\n",
        "        x, y = x_pos_encoding + x, y_pos_encoding + y\n",
        "        x, y = self.dropout1(x), self.dropout2(y)\n",
        "        encoderOutput = self.encoderStack(x, x_mask)\n",
        "        decoderOutput = self.decoderStack(encoderOutput, y, y_mask)\n",
        "        logits = self.generate(decoderOutput)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "n5c2hP21MjIp"
      },
      "outputs": [],
      "source": [
        "model = Transformer(d_model=D_MODEL, d_h=D_H, d_ff=D_FF, embedding_size=EMBEDDING_SIZE, vocab_size=VOCAB_SIZE, dropout=DROPOUT, num_coder_layers=N)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4DglT1Wd8LM0"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def compute_test_loss(model: nn.Module, test_data: DataLoader, padding_index: int):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=padding_index)\n",
        "    loss = 0\n",
        "    for src, tgt in test_data:\n",
        "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "        tgt_in = tgt[:, :-1]\n",
        "        tgt_out = tgt[:, 1:]\n",
        "        logits = model(src, tgt_in).permute(0, 2, 1)\n",
        "        loss += criterion(logits, tgt_out).item()\n",
        "    average_loss = loss / len(list(test_data))\n",
        "    model.train()\n",
        "    return average_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3OfIu8IFjmbt"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, train_data: DataLoader, test_data: DataLoader, learning_rate: int, padding_idx: int, epoch_num: int):\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
        "\n",
        "    batch_train_loss = []\n",
        "    epoch_test_loss = []\n",
        "    epoch_train_loss = []\n",
        "    for epoch in range(0, epoch_num):\n",
        "\n",
        "        print(\"#\"*67)\n",
        "        print(f'{\"#\"*20}Training begins for epoc {epoch:>2}{\"#\"*20}')\n",
        "        print(\"#\"*67)\n",
        "        data_len = len(list(train_data))\n",
        "        epoch_train_loss = 0\n",
        "        total_loss = 0\n",
        "        for i, (src, tgt) in enumerate(train_data):\n",
        "\n",
        "            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "            tgt_in = tgt[:, :-1] # inputs shifted left\n",
        "            logits = model(src, tgt_in).permute(0, 2, 1)\n",
        "            optimizer.zero_grad()\n",
        "            tgt_out = tgt[:, 1:] # labels shifted right\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(logits, tgt_out)\n",
        "            total_loss += loss.item()\n",
        "            epoch_train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i%(data_len // 3) == 0 and i != 0:\n",
        "                loss_per_batch = total_loss / 100\n",
        "                print(f'Loss per batch is: {loss_per_batch}')\n",
        "                batch_train_loss.append(loss_per_batch)\n",
        "                total_loss = 0\n",
        "        epoch_train_loss /= len(list(train_data))\n",
        "        print(f'Training is complete for epoch {epoch:>2}, average training loss for epoch {epoch:>2}: {epoch_train_loss:>5.3f}')\n",
        "        test_loss = compute_test_loss(model, test_data, padding_idx)\n",
        "        epoch_test_loss.append(test_loss)\n",
        "        print(f'Test loss for epoch {epoch:>2} is: {test_loss:>5.3f}')\n",
        "        print(\"-\"*67)\n",
        "    plt.plot(batch_train_loss, color='blue')\n",
        "    plt.title(\"Batch Loss Graph\")\n",
        "    plt.xlabel(\"Hundred Batches\")\n",
        "    plt.ylabel(\"Loss per Batch\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epoch_test_loss, color='purple', label='Epoch Test Loss')\n",
        "    plt.plot(epoch_train_loss, color='green', label='Epoch Train Loss')\n",
        "    plt.title(\"Epoch Loss Graph\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JEYHRiFeZVwT",
        "outputId": "645bff19-a7a6-4967-e5a1-5f4f4bc03679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###################################################################\n",
            "####################Training begins for epoc  0####################\n",
            "###################################################################\n",
            "Loss per batch is: 6.2038782787323\n",
            "Loss per batch is: 4.615228319168091\n",
            "Loss per batch is: 4.303574073314667\n",
            "Loss per batch is: 4.150267951488495\n",
            "Training is complete for epoch  0, average training loss for epoch  0: 4.753\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc  1####################\n",
            "###################################################################\n",
            "Loss per batch is: 3.81856929063797\n",
            "Loss per batch is: 3.660170955657959\n",
            "Loss per batch is: 3.623656494617462\n",
            "Loss per batch is: 3.598783197402954\n",
            "Training is complete for epoch  1, average training loss for epoch  1: 3.686\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc  2####################\n",
            "###################################################################\n",
            "Loss per batch is: 3.361877784729004\n",
            "Loss per batch is: 3.2317821741104127\n",
            "Loss per batch is: 3.230597684383392\n",
            "Loss per batch is: 3.242667796611786\n",
            "Training is complete for epoch  2, average training loss for epoch  2: 3.285\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc  3####################\n",
            "###################################################################\n",
            "Loss per batch is: 3.0224508833885193\n",
            "Loss per batch is: 2.9132838153839113\n",
            "Loss per batch is: 2.944043843746185\n",
            "Loss per batch is: 2.9733706068992616\n",
            "Training is complete for epoch  3, average training loss for epoch  3: 2.987\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc  4####################\n",
            "###################################################################\n",
            "Loss per batch is: 2.7595341563224793\n",
            "Loss per batch is: 2.661808280944824\n",
            "Loss per batch is: 2.712313311100006\n",
            "Loss per batch is: 2.754118025302887\n",
            "Training is complete for epoch  4, average training loss for epoch  4: 2.749\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc  5####################\n",
            "###################################################################\n",
            "Loss per batch is: 2.542979619503021\n",
            "Loss per batch is: 2.457089197635651\n",
            "Loss per batch is: 2.5205354356765746\n",
            "Loss per batch is: 2.574606883525848\n",
            "Training is complete for epoch  5, average training loss for epoch  5: 2.553\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc  6####################\n",
            "###################################################################\n",
            "Loss per batch is: 2.364945991039276\n",
            "Loss per batch is: 2.284925379753113\n",
            "Loss per batch is: 2.3467215955257417\n",
            "Loss per batch is: 2.4136372876167296\n",
            "Training is complete for epoch  6, average training loss for epoch  6: 2.382\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc  7####################\n",
            "###################################################################\n",
            "Loss per batch is: 2.2010349786281584\n",
            "Loss per batch is: 2.130369920730591\n",
            "Loss per batch is: 2.197050691843033\n",
            "Loss per batch is: 2.2633147656917574\n",
            "Training is complete for epoch  7, average training loss for epoch  7: 2.229\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc  8####################\n",
            "###################################################################\n",
            "Loss per batch is: 2.061135448217392\n",
            "Loss per batch is: 1.981514768600464\n",
            "Loss per batch is: 2.0605963957309723\n",
            "Loss per batch is: 2.1336056530475616\n",
            "Training is complete for epoch  8, average training loss for epoch  8: 2.090\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc  9####################\n",
            "###################################################################\n",
            "Loss per batch is: 1.9401771545410156\n",
            "Loss per batch is: 1.8516914522647858\n",
            "Loss per batch is: 1.936200464963913\n",
            "Loss per batch is: 2.0071595072746278\n",
            "Training is complete for epoch  9, average training loss for epoch  9: 1.963\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc 10####################\n",
            "###################################################################\n",
            "Loss per batch is: 1.8136180830001831\n",
            "Loss per batch is: 1.7316959071159364\n",
            "Loss per batch is: 1.8075914752483369\n",
            "Loss per batch is: 1.8876464474201202\n",
            "Training is complete for epoch 10, average training loss for epoch 10: 1.841\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc 11####################\n",
            "###################################################################\n",
            "Loss per batch is: 1.6928268074989319\n",
            "Loss per batch is: 1.6190174698829651\n",
            "Loss per batch is: 1.7038679349422454\n",
            "Loss per batch is: 1.7854138827323913\n",
            "Training is complete for epoch 11, average training loss for epoch 11: 1.729\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc 12####################\n",
            "###################################################################\n",
            "Loss per batch is: 1.586186261177063\n",
            "Loss per batch is: 1.5199956214427948\n",
            "Loss per batch is: 1.6076196753978729\n",
            "Loss per batch is: 1.683722597360611\n",
            "Training is complete for epoch 12, average training loss for epoch 12: 1.628\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc 13####################\n",
            "###################################################################\n",
            "Loss per batch is: 1.4884774005413055\n",
            "Loss per batch is: 1.4280411076545716\n",
            "Loss per batch is: 1.5132232892513275\n",
            "Loss per batch is: 1.5810234260559082\n",
            "Training is complete for epoch 13, average training loss for epoch 13: 1.532\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc 14####################\n",
            "###################################################################\n",
            "Loss per batch is: 1.4068741869926453\n",
            "Loss per batch is: 1.3384001302719115\n",
            "Loss per batch is: 1.414697253704071\n",
            "Loss per batch is: 1.4815168011188506\n",
            "Training is complete for epoch 14, average training loss for epoch 14: 1.439\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc 15####################\n",
            "###################################################################\n",
            "Loss per batch is: 1.3205852735042571\n",
            "Loss per batch is: 1.2504526352882386\n",
            "Loss per batch is: 1.3328656524419784\n",
            "Loss per batch is: 1.398589049577713\n",
            "Training is complete for epoch 15, average training loss for epoch 15: 1.353\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc 16####################\n",
            "###################################################################\n",
            "Loss per batch is: 1.2418882340192794\n",
            "Loss per batch is: 1.1632667195796966\n",
            "Loss per batch is: 1.2481547337770462\n",
            "Loss per batch is: 1.3206479561328888\n",
            "Training is complete for epoch 16, average training loss for epoch 16: 1.271\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc 17####################\n",
            "###################################################################\n",
            "Loss per batch is: 1.1564910972118378\n",
            "Loss per batch is: 1.0842398411035539\n",
            "Loss per batch is: 1.176401771903038\n",
            "Loss per batch is: 1.2498995780944824\n",
            "Training is complete for epoch 17, average training loss for epoch 17: 1.196\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc 18####################\n",
            "###################################################################\n",
            "Loss per batch is: 1.0746901792287826\n",
            "Loss per batch is: 1.0127249729633332\n",
            "Loss per batch is: 1.1025779962539672\n",
            "Loss per batch is: 1.1743565332889556\n",
            "Training is complete for epoch 18, average training loss for epoch 18: 1.119\n",
            "-------------------------------------------------------------------\n",
            "###################################################################\n",
            "####################Training begins for epoc 19####################\n",
            "###################################################################\n",
            "Loss per batch is: 1.0056528425216675\n",
            "Loss per batch is: 0.9636704748868943\n",
            "Loss per batch is: 1.0343949788808822\n",
            "Loss per batch is: 1.08688364982605\n",
            "Training is complete for epoch 19, average training loss for epoch 19: 1.048\n",
            "-------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "train(model, train_dataloader, test_dataloader, learning_rate=LR, padding_idx=PAD_IDX, epoch_num=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL1Ka42zXlfq",
        "outputId": "e5bb922b-d73e-4a98-be52-f3d3dfd8e431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 64])\n",
            "torch.Size([64, 64])\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "src, tgt = next(iter(test_dataloader))\n",
        "src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "print(src.shape)\n",
        "print(tgt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WI6E17_CXlfr"
      },
      "outputs": [],
      "source": [
        "logits = model(src, tgt)\n",
        "pred = torch.argmax(logits, dim=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtqy9lxxXlfr",
        "outputId": "b9cee4e0-df00-4e8e-8491-5e039ce7993e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example no 0: \n",
            "################################################################################\n",
            "#German text:  Eine Gruppe von Mnnern ldt Baumwolle auf einen Lastwagen\n",
            "################################################################################\n",
            "#English translation:  A group of men are loading cotton onto a truck\n",
            "################################################################################\n",
            "#Model translation:  A group of men loading swinging up a a truck . a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 1: \n",
            "################################################################################\n",
            "#German text:  Ein Mann schlft in einem grnen Raum auf einem Sofa .\n",
            "################################################################################\n",
            "#English translation:  A man sleeping in a green room on a couch .\n",
            "################################################################################\n",
            "#Model translation:  A man is on a green room . a couch . . on . . . . . on on . . . . . . . . . . . . . . . . . . . . . . . . . on on . . . . . on . . . . . . on . . .\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 2: \n",
            "################################################################################\n",
            "#German text:  Ein Junge mit Kopfhrern sitzt auf den Schultern einer Frau .\n",
            "################################################################################\n",
            "#English translation:  A boy wearing headphones sits on a woman ' s shoulders .\n",
            "################################################################################\n",
            "#Model translation:  A young with earphones on on a couch ' s shoulders . on her her her her her a her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her her\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 3: \n",
            "################################################################################\n",
            "#German text:  Zwei Mnner bauen eine blaue Eis f ischer htte auf einem zuge froren en See auf\n",
            "################################################################################\n",
            "#English translation:  Two men setting up a blue ice fishing hut on an iced over lake\n",
            "################################################################################\n",
            "#Model translation:  Two guys on up a blue track skate pole on a outdoor tea a in . . . . a a a a . a water water water water a a a . . a . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 4: \n",
            "################################################################################\n",
            "#German text:  Ein Mann mit beginnender Glatze , der eine rote Rettungsweste trgt , sitzt in einem kleinen Boot .\n",
            "################################################################################\n",
            "#English translation:  A balding man wearing a red life jacket is sitting in a small boat .\n",
            "################################################################################\n",
            "#Model translation:  A balding man is all red life jacket is sitting in a small boat . in a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 5: \n",
            "################################################################################\n",
            "#German text:  Eine Frau in einem rotem Mantel , die eine vermutlich aus Asien st ammen de Handtasche in einem blauen Farbton hlt , springt fr einen Schnappschuss in die Luft .\n",
            "################################################################################\n",
            "#English translation:  A lady in a red coat , holding a blu ish hand bag likely of asian descent , jumping off the ground for a snap shot .\n",
            "################################################################################\n",
            "#Model translation:  A woman wearing a red coat , holds a clothesline ish , in , in clothes culture jump jumps in in air . blue pose in in in - - - . . . . in - in in in in in . . . . - - - - . . . . . . . . . . . the .\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 6: \n",
            "################################################################################\n",
            "#German text:  Ein brauner Hund rennt dem schwarzen Hund hinterher .\n",
            "################################################################################\n",
            "#English translation:  A brown dog is running after the black dog .\n",
            "################################################################################\n",
            "#Model translation:  A brown dog chases running after the black dog . . - . dog . the the the the the . dog the the dog dog dog dog dog the the dog dog dog dog the . . the the the the the the the . the the the the the the the the the the the the dog . . dog dog\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 7: \n",
            "################################################################################\n",
            "#German text:  Ein kleiner Junge mit einem Giants - Trikot schwingt einen Baseballschlger in Richtung eines ankommenden Balls .\n",
            "################################################################################\n",
            "#English translation:  A young boy wearing a Giants jersey swings a baseball bat at an incoming pitch .\n",
            "################################################################################\n",
            "#Model translation:  A little boy in a brown , is a baseball bat at a bat baseball . . a a a a a a a a a a a a a a a a a a a a a a a a a a a a a . a a a a a a a the the a a a a . . a\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 8: \n",
            "################################################################################\n",
            "#German text:  Ein Mann telefoniert in einem unaufgerumten Bro\n",
            "################################################################################\n",
            "#English translation:  A man in a cluttered office is using the telephone\n",
            "################################################################################\n",
            "#Model translation:  A man is an black office is in his same booth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "################################################################################\n",
            "\n",
            "\n",
            "Example no 9: \n",
            "################################################################################\n",
            "#German text:  Eine lchelnde Frau mit einem pfirsichfarbenen Trgershirt hlt ein Mountainbike\n",
            "################################################################################\n",
            "#English translation:  A smiling woman in a peach tank top stands holding a mountain bike\n",
            "################################################################################\n",
            "#Model translation:  A smiling woman with a skiing , top is in a mountain . . a a a a a a a a her her her a - her her her her her her . . her her her . . . her her her her her her her her her her her her her . . . her her . her her her\n",
            "################################################################################\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for example in range(10):\n",
        "    print(f\"Example no {example}: \")\n",
        "    '''pad_count = (tgt[example] == PAD_IDX).data.sum().item()\n",
        "    sentence_len = tgt.shape[1] - pad_count'''\n",
        "    print(\"#\"*80)\n",
        "    print(\"#German text: \", tokenizer.decode(src[example].cpu().numpy().squeeze()))\n",
        "    print(\"#\"*80)\n",
        "    print(\"#English translation: \", tokenizer.decode(tgt[example].cpu().numpy().squeeze()))\n",
        "    print(\"#\"*80)\n",
        "    print(\"#Model translation: \", tokenizer.decode(pred[example].cpu().numpy().squeeze()))\n",
        "    print(\"#\"*80)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taRyHEWTedXi",
        "outputId": "985442a2-2dbd-4f60-f0f1-e7cad95be67a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 64])\n",
            "torch.Size([1, 64])\n"
          ]
        }
      ],
      "source": [
        "src, tgt = next(iter(test_dataloader))\n",
        "src = src[:1]\n",
        "tgt = tgt[:1]\n",
        "print(src.shape)\n",
        "print(tgt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPgMJ93d25w7",
        "outputId": "52a6410e-4420-4c94-c88a-42345822f538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A group of men loading a truck\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def translate(model: nn.Module, source: Tensor, start_token: int, stop_token: int, seq_len: int):\n",
        "    model.eval()\n",
        "\n",
        "    source = source.to(DEVICE)\n",
        "    src_batch_dim = source.shape[0]\n",
        "    src_seq_len = source.shape[1]\n",
        "\n",
        "    src_mask = model.masking(src_batch_dim, src_seq_len)\n",
        "    src_pos_enc = model.positional(src_seq_len)\n",
        "    src_embedding = model.embed(source)\n",
        "\n",
        "    encoderInput = src_embedding + src_pos_enc\n",
        "\n",
        "    encoderOutput = model.encoderStack(encoderInput, src_mask)\n",
        "\n",
        "    ys = torch.tensor([[start_token]], dtype=torch.int32, device=DEVICE)\n",
        "\n",
        "    for _ in range(seq_len):\n",
        "\n",
        "        tgt_batch_dim = ys.shape[0]\n",
        "        tgt_seq_len = ys.shape[1]\n",
        "\n",
        "        embedded_sequence = model.embed(ys)\n",
        "        seq_pos_enc = model.positional(tgt_seq_len)\n",
        "        decoder_mask = model.masking(tgt_batch_dim, tgt_seq_len)\n",
        "\n",
        "        decoderInput = embedded_sequence + seq_pos_enc\n",
        "    \n",
        "        out = model.decoderStack(encoderOutput, decoderInput, decoder_mask)\n",
        "\n",
        "        logits = model.generate(out[:, -1, :])\n",
        "        generated_word_id = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        ys = torch.cat((ys, torch.tensor([[generated_word_id.item()]], \n",
        "                                                     dtype=torch.int32, device=DEVICE)), dim=1)\n",
        "\n",
        "        if generated_word_id == stop_token:\n",
        "            break\n",
        "\n",
        "    translation_ids = ys.cpu().numpy()\n",
        "\n",
        "    translation = tokenizer.decode(translation_ids.squeeze())\n",
        "    return translation\n",
        "\n",
        "translation = translate(model=model, source=src, start_token=BOS_IDX, stop_token=EOS_IDX, seq_len=MAX_LEN)\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR0Q2WoxmlKJ",
        "outputId": "25b383f2-2b42-4525-e014-9f9cae8e247f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "German:  Auf der Veranda spielt ein glcklicher Hund.\n",
            "English:  A happy dog is playing on the porch.\n",
            "Model translation:  A dog is playing on the dog .\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Auf der Veranda spielt ein glcklicher Hund.\"\n",
        "encoded_german = tokenizer.encode(sentence)\n",
        "german_id_arr = encoded_german.ids\n",
        "\n",
        "german = torch.tensor(german_id_arr).unsqueeze(0)\n",
        "english = \"A happy dog is playing on the porch.\"\n",
        "translation = translate(model, german, 2, 3, MAX_LEN)\n",
        "print(\"German: \", sentence)\n",
        "print(\"English: \", english)\n",
        "print(\"Model translation: \", translation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6fb3815109fae185a2989dc67116c03680cdd5befd2823d06e43fcf705655cc7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
